{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Decision Tree | Assignment**"
      ],
      "metadata": {
        "id": "zFsq1hb5Kemd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1: What is a Decision Tree, and how does it work in the context of** **classification?**\n",
        "\n",
        "---\n",
        "Ans: A **Decision Tree** is a simple model used in machine learning to make predictions.  \n",
        "In classification, it is used to put data into different groups or categories.  \n",
        "\n",
        "It works like a flowchart:\n",
        "- The tree starts at the root node, which is the first question about the data.  \n",
        "- Based on the answer, the data follows a branch to the next question.  \n",
        "- This continues until the data reaches a leaf node, which gives the final class or result.  \n",
        "\n",
        "For example, to decide if a fruit is an apple or an orange, the tree might first ask about the color.  \n",
        "If it is red, the tree predicts \"apple.\" If it is orange in color, the tree predicts \"orange.\"  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BNjhb4-bKhU4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2: Explain the concepts of Gini Impurity and Entropy as impurity measures**. **How do they impact the splits in a Decision Tree?**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "When building a Decision Tree, the algorithm needs a way to decide which feature\n",
        "and value should be used to split the data. For this, it uses impurity measures.\n",
        "Two common measures are Gini Impurity and Entropy.\n",
        "\n",
        "**1. Gini Impurity**  \n",
        "- Gini shows how often a randomly chosen element from the dataset\n",
        "would be labeled incorrectly if it was randomly classified.  \n",
        "- A Gini value of 0 means the node is pure (all data belongs to one class).  \n",
        "- Higher values mean the data is mixed between classes.  \n",
        "\n",
        "**2. Entropy**  \n",
        "- Entropy comes from information theory. It measures the \"disorder\" or \"uncertainty\" in the data.  \n",
        "- Entropy is 0 when all examples belong to the same class.  \n",
        "- Entropy is higher when the classes are more evenly mixed.  \n",
        "\n",
        "**Impact on Splits**  \n",
        "Both Gini and Entropy are used to find the \"best split\" in the data:  \n",
        "- The Decision Tree looks for the feature and threshold that give the greatest reduction in impurity.  \n",
        "- The lower the impurity after the split, the better the split.  \n",
        "- In practice, both Gini and Entropy often lead to very similar trees, but Gini is a bit faster to compute.  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "soaAPUTzKime"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3: What is the difference between Pre-Pruning and Post-Pruning in Decision**\n",
        "**Trees? Give one practical advantage of using each**\n",
        "\n",
        "---\n",
        "Ans:\n",
        "**Pre-Pruning**  \n",
        "Pre-pruning stops the growth of a Decision Tree early by setting limits, such as maximum depth or minimum number of samples required to split.  \n",
        "- *Advantage:* It saves time and prevents the tree from becoming too large and complex.  \n",
        "\n",
        "**Post-Pruning**  \n",
        "Post-pruning allows the tree to grow fully and then removes the branches that do not add much value. This makes the tree simpler and avoids overfitting.  \n",
        "- *Advantage:* It improves accuracy on new data by reducing overfitting.  \n",
        "\n"
      ],
      "metadata": {
        "id": "BYE4XPG5KkUT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4: What is Information Gain in Decision Trees, and why is it important for**\n",
        "**choosing the best split?**\n",
        "\n",
        "---\n",
        "Ans: **Information Gain** measures how much a feature helps to reduce uncertainty or disorder (entropy) in the data.  \n",
        "- When a Decision Tree considers a split, it calculates the reduction in entropy after splitting the data using that feature.  \n",
        "- The feature that gives the highest information gain is chosen for the split because it best separates the classes.  \n",
        "\n",
        "**Why it is important:**  \n",
        "Information Gain helps the tree make the most useful splits, which leads to more accurate predictions and a simpler tree structure.\n",
        "\n"
      ],
      "metadata": {
        "id": "5LFCcn_oKkQ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5: What are some common real-world applications of Decision Trees, and**\n",
        "**what are their main advantages and limitations?**\n",
        "\n",
        "---\n",
        "Ans:**Applications of Decision Trees:**  \n",
        "- **Medical diagnosis:** Predicting diseases based on symptoms and test results.  \n",
        "- **Finance:** Credit scoring and loan approval decisions.  \n",
        "- **Marketing:** Customer segmentation and predicting purchase behavior.  \n",
        "- **Engineering:** Fault detection and quality control.  \n",
        "\n",
        "**Advantages:**  \n",
        "- Easy to understand and interpret.  \n",
        "- Can handle both numerical and categorical data.  \n",
        "- Requires little data preparation.  \n",
        "\n",
        "**Limitations:**  \n",
        "- Can easily overfit the data if the tree grows too deep.  \n",
        "- Sensitive to small changes in data.  \n",
        "- Sometimes less accurate compared to other complex models like Random Forests or Gradient Boosting.  \n",
        "\n"
      ],
      "metadata": {
        "id": "DMubSNzGKkOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info:\n",
        "# ● Iris Dataset for classification tasks (sklearn.datasets.load_iris() or provided CSV).\n",
        "# ● Boston Housing Dataset for regression tasks (sklearn.datasets.load_boston() or provided CSV).\n",
        "# Question 6: Write a Python program to:\n",
        "# ● Load the Iris Dataset\n",
        "# ● Train a Decision Tree Classifier using the Gini criterion\n",
        "# ● Print the model’s accuracy and feature importances\n",
        "\n",
        "# Ans:\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the Decision Tree Classifier: {accuracy:.2f}\")\n",
        "\n",
        "for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"Feature: {feature}, Importance: {importance:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wmmKFQ9OVuO",
        "outputId": "e91f6350-eebc-47ad-ad9e-5412d61d7dbd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the Decision Tree Classifier: 1.00\n",
            "Feature: sepal length (cm), Importance: 0.00\n",
            "Feature: sepal width (cm), Importance: 0.02\n",
            "Feature: petal length (cm), Importance: 0.91\n",
            "Feature: petal width (cm), Importance: 0.08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7: Write a Python program to:\n",
        "# ● Load the Iris Dataset\n",
        "# ● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree.\n",
        "\n",
        "# Ans:\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "full_tree = DecisionTreeClassifier(random_state=42)\n",
        "full_tree.fit(X_train, y_train)\n",
        "y_pred_full = full_tree.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "limited_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "limited_tree.fit(X_train, y_train)\n",
        "y_pred_limited = limited_tree.predict(X_test)\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "print(f\"Accuracy of fully-grown tree: {accuracy_full:.2f}\")\n",
        "print(f\"Accuracy of tree with max_depth=3: {accuracy_limited:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nznoSS1GQqbz",
        "outputId": "f40692f1-9e1f-463f-e875-852e9563e17a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of fully-grown tree: 1.00\n",
            "Accuracy of tree with max_depth=3: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8: Write a Python program to:\n",
        "# ● Load the California Housing dataset from sklearn\n",
        "# ● Train a Decision Tree Regressor\n",
        "# ● Print the Mean Squared Error (MSE) and feature importances\n",
        "\n",
        "# Ans:\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "\n",
        "for feature, importance in zip(data.feature_names, regressor.feature_importances_):\n",
        "    print(f\"Feature: {feature}, Importance: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEC-tKQGREXt",
        "outputId": "fc3b61d1-6282-4bba-d7fe-c45c67c2d0e8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.50\n",
            "Feature: MedInc, Importance: 0.5285\n",
            "Feature: HouseAge, Importance: 0.0519\n",
            "Feature: AveRooms, Importance: 0.0530\n",
            "Feature: AveBedrms, Importance: 0.0287\n",
            "Feature: Population, Importance: 0.0305\n",
            "Feature: AveOccup, Importance: 0.1308\n",
            "Feature: Latitude, Importance: 0.0937\n",
            "Feature: Longitude, Importance: 0.0829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9: Write a Python program to:\n",
        "# ● Load the Iris Dataset\n",
        "# ● Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV\n",
        "# ● Print the best parameters and the resulting model accuracy\n",
        "\n",
        "# Ans:\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "param_grid = {'max_depth': [2, 3, 4, 5, None],'min_samples_split': [2, 5, 10]}\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "grid_search = GridSearchCV(dt, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"Accuracy of the tuned Decision Tree: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tg6kkq3sRiUa",
        "outputId": "622a6949-1f69-4607-f311-dd6c75f79591"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Accuracy of the tuned Decision Tree: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10: Imagine you’re working as a data scientist for a healthcare company that**\n",
        "**wants to predict whether a patient has a certain disease. You have a large** **dataset with**\n",
        "**mixed data types and some missing values**.\n",
        "**Explain the step-by-step process you would follow to**:\n",
        "\n",
        "●**Handle the missing values**\n",
        "\n",
        "● **Encode the categorical features**\n",
        "\n",
        "● **Train a Decision Tree model**\n",
        "\n",
        "● **Tune its hyperparameters**\n",
        "\n",
        "● **Evaluate its performance**\n",
        "\n",
        "**And describe what business value this model could provide in the real-world**\n",
        "**setting**.\n",
        "\n",
        "---\n",
        "Ans: **1. Handle Missing Values:**  \n",
        "- Identify which columns have missing data.  \n",
        "- For numerical features, replace missing values with the mean or median.  \n",
        "- For categorical features, replace missing values with the mode or create a special category like \"Unknown\".  \n",
        "\n",
        "**2. Encode Categorical Features:**  \n",
        "- Convert categorical features into numeric form so the Decision Tree can use them.  \n",
        "- Use techniques like **one-hot encoding** for nominal variables or **label encoding** for ordinal variables.  \n",
        "\n",
        "**3. Train a Decision Tree Model:**  \n",
        "- Split the dataset into **training and testing sets**.  \n",
        "- Train a Decision Tree classifier on the training data.  \n",
        "- Use default parameters first to get a baseline performance.  \n",
        "\n",
        "**4. Tune Hyperparameters:**  \n",
        "- Adjust parameters like max_depth, min_samples_split, or max_features to prevent overfitting and improve accuracy.  \n",
        "- Use GridSearchCV or RandomizedSearchCV to find the best combination of parameters.  \n",
        "\n",
        "**5. Evaluate Performance:**  \n",
        "- Measure the model’s accuracy, precision, recall, and F1-score on the test set.  \n",
        "- Use a confusion matrix to see how well the model predicts positive and negative cases.  \n",
        "\n",
        "**Business Value:**  \n",
        "- The model can help doctors identify patients at risk of the disease early.  \n",
        "- It supports better resource allocation by focusing attention on high-risk patients.  \n",
        "- It can reduce healthcare costs by enabling preventive measures.  \n",
        "- Overall, it improves patient outcomes and operational efficiency.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XZn3JdBlQqEH"
      }
    }
  ]
}